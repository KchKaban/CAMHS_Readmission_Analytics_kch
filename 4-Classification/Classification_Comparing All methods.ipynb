{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have first the latest and last solution we applied for classification. <br>\n",
    "After preparation, we made Binary and Multi-class classifiers with different algorithms:  DecisionTreeClassifier, LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, MLPClassifier <br>\n",
    "Also we compared each of them in 4 versions: <br>\n",
    "1 - without appling any weighting or oversamplling <br>\n",
    "2 - with applying custom weights <br>\n",
    "3 - with applying class weights <br>\n",
    "4 - with oversamplelling <br>\n",
    "\n",
    "Finally we chose, XGBClassifier with class weight for our Binary classifier, final Logistic Regression with oversampling for our final Multi-class classifier <br>\n",
    "\n",
    "At the end one more time we showed our inital solution which was flat classification with the following labels: \n",
    "labels' value count: label <br>\n",
    "5    19250 <br>\n",
    "0      878 <br>\n",
    "1      721 <br>\n",
    "2      718 <br>\n",
    "4      662 <br>\n",
    "3      447 <br>\n",
    "In this solution we could not apply efficiantly apply the methods for solving imbalance issues.<br>\n",
    "That led us to have instead two different classifiers (binary, multi-class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary and Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['episode_id', 'tillnextepisode', 'Length_of_Episode', 'Count_visit',\n",
      "       'SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
      "       'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio',\n",
      "       ...\n",
      "       'G03DA02', 'B03BA03', 'G03FB05', 'S01AA13', 'A11JB', 'M01AE01',\n",
      "       'N05AE04', 'N06AA09', 'R03AC03', 'B03BA01'],\n",
      "      dtype='object', length=218)\n",
      "here: 22676\n",
      "here: 3426\n",
      "22676\n",
      "binary_label\n",
      "0    19250\n",
      "1     3426\n",
      "Name: count, dtype: int64\n",
      "[0 1]\n",
      "19250 3426\n",
      "0 with 19250\n",
      "3426\n",
      "multi_label\n",
      "1    1316\n",
      "2    1109\n",
      "0    1001\n",
      "Name: count, dtype: int64\n",
      "[1 2 0]\n",
      "1316\n",
      "1 with 1316\n",
      "22676 Index(['tillnextepisode', 'Length_of_Episode', 'Count_visit',\n",
      "       'SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
      "       'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio',\n",
      "       'TreatmentPlanning_ratio',\n",
      "       ...\n",
      "       'B03BA03', 'G03FB05', 'S01AA13', 'A11JB', 'M01AE01', 'N05AE04',\n",
      "       'N06AA09', 'R03AC03', 'B03BA01', 'binary_label'],\n",
      "      dtype='object', length=218)\n",
      "3426 Index(['tillnextepisode', 'Length_of_Episode', 'Count_visit',\n",
      "       'SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
      "       'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio',\n",
      "       'TreatmentPlanning_ratio',\n",
      "       ...\n",
      "       'G03FB05', 'S01AA13', 'A11JB', 'M01AE01', 'N05AE04', 'N06AA09',\n",
      "       'R03AC03', 'B03BA01', 'binary_label', 'multi_label'],\n",
      "      dtype='object', length=219)\n",
      "22676\n",
      "3426\n"
     ]
    }
   ],
   "source": [
    "# read data *****\n",
    "final_episodes = pd.read_csv(\"/home/kabank/workbench/.conda/analysis/kabank-data/final_episodes.csv\")\n",
    "fullHot_episodes = pd.read_csv(\"/home/kabank/workbench/.conda/analysis/kabank-data/final_episodes4.csv\") \n",
    "Diag_med = pd.read_csv('/home/kabank/workbench/.conda/analysis/kabank-data/new-data/Dummies_ICD10_ATC_100.csv') \n",
    "# required changes *****\n",
    "final_episodes.rename(columns={'var_no_dates_permonth': 'SD_CareEvent_PerMonth'}, inplace=True)\n",
    "\n",
    "#merge to add some required columns *****\n",
    "final_episodes = pd.merge(final_episodes, fullHot_episodes[['episode_id', 'num_diagnoses', 'num_medications', 'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']], on='episode_id', how='inner')\n",
    "final_episodes = pd.merge(final_episodes[['episode_id','tillnextepisode','Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']], Diag_med, on='episode_id', how='left')\n",
    "\n",
    "# features = ['episode_id','Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "#        'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "#        'gender_0','F','M','MiddleChildhood','Preschooler','Teenager','tillnextepisode']\n",
    "features = final_episodes.iloc[:,:].columns\n",
    "print(features)\n",
    "#final_episodes1 = final_episodes[features]\n",
    "\n",
    "\n",
    "#labeling 0 and 1 for not_readmitted and readmitted\n",
    "for i in range(len(final_episodes['tillnextepisode'])):\n",
    "    if np.isnan(final_episodes.loc[i, 'tillnextepisode']):\n",
    "        final_episodes.loc[i, 'binary_label'] = 0\n",
    "    else:\n",
    "        final_episodes.loc[i, 'binary_label'] = 1\n",
    "    \n",
    "final_episodes['binary_label'] = final_episodes['binary_label'].astype(int)\n",
    "\n",
    "final_episodes1_episode_id = pd.DataFrame()\n",
    "final_episodes1_episode_id['episode_id'] = final_episodes['episode_id'].astype(int)\n",
    "#final_episodes1 = final_episodes[final_episodes.iloc[1,:].columns]\n",
    "final_episodes1 = final_episodes.iloc[:, 1:]\n",
    "\n",
    "final_episodes2_episode_id = pd.DataFrame()\n",
    "final_episodes2_episode_id['episode_id'] = final_episodes[final_episodes['binary_label']==1]['episode_id'].astype(int)\n",
    "final_episodes2 = final_episodes[final_episodes['binary_label']==1].iloc[:, 1:]\n",
    "print('here:',final_episodes['episode_id'].nunique())\n",
    "print('here:',final_episodes2_episode_id['episode_id'].nunique())\n",
    "# Encoding 'tillnextepisode' into labels:  \n",
    "le = LabelEncoder()\n",
    "#First attempt with 6 labels and flat classification\n",
    "# final_episodes2['multi_label'] = le.fit_transform(pd.cut(final_episodes2['tillnextepisode'],\n",
    "#                                                   bins=[float('-inf'), 0, 180, 365, 730, 1095, float('inf')],\n",
    "#                                                   labels=['not-re-admitted',\n",
    "#                                                           're-admitted in 0-180 days',\n",
    "#                                                           're-admitted in 180-365 days',\n",
    "#                                                           're-admitted in 365-730 days',\n",
    "#                                                           're-admitted in 730-1095 days',\n",
    "#                                                           're-admitted in more than 1095 days']))\n",
    "\n",
    "# test these: \n",
    "# bins=[float('-inf'), 0, 182, 730, float('inf')]\n",
    "# bins=[float('-inf'), 0, 273, 730, float('inf')]\n",
    "# bins=[float('-inf'), 0, 365, 730, float('inf')]\n",
    "final_episodes2['multi_label'] = le.fit_transform(pd.cut(final_episodes2['tillnextepisode'],\n",
    "                                                  bins=[float('-inf'), 0, 182, 730, float('inf')],\n",
    "                                                  labels=['not-re-admitted',\n",
    "                                                          '0-6m',\n",
    "                                                          '6m-2y',\n",
    "                                                          'over 2y']))\n",
    "\n",
    "# final_episodes2['multi_label'] = le.fit_transform(pd.cut(final_episodes2['tillnextepisode'],\n",
    "#                                                   bins=[float('-inf'), 0, 273, 730, float('inf')],\n",
    "#                                                   labels=['not-re-admitted',\n",
    "#                                                           '0-9m',\n",
    "#                                                           '9m-2y',\n",
    "#                                                           'over 2y']))\n",
    "\n",
    "# final_episodes2['multi_label'] = le.fit_transform(pd.cut(final_episodes2['tillnextepisode'],\n",
    "#                                                   bins=[float('-inf'), 0, 365, 730, float('inf')],\n",
    "#                                                   labels=['not-re-admitted',\n",
    "#                                                           '0-1y',\n",
    "#                                                           '1y-2y',\n",
    "#                                                           'over 2y']))\n",
    "\n",
    "\n",
    "\n",
    "# for binary classification: admitted: 1/ not-readmitted: 0\n",
    "#print(final_episodes1.columns)\n",
    "print(len(final_episodes1))\n",
    "print(final_episodes1['binary_label'].value_counts())\n",
    "print(final_episodes1['binary_label'].unique())\n",
    "counts = final_episodes1['binary_label'].value_counts()\n",
    "print(counts[0], counts[1])\n",
    "max_val_bin = counts.max()\n",
    "print(counts.idxmax(),'with', max_val_bin)\n",
    "\n",
    "print(len(final_episodes2))\n",
    "print(final_episodes2['multi_label'].value_counts())\n",
    "print(final_episodes2['multi_label'].unique())\n",
    "counts = final_episodes2['multi_label'].value_counts()\n",
    "max_val_multi = counts.max()\n",
    "print(max_val_multi)\n",
    "print(counts.idxmax(),'with', max_val_multi)\n",
    "\n",
    "print(len(final_episodes1), final_episodes1.columns)\n",
    "print(len(final_episodes2), final_episodes2.columns)\n",
    "print(len(final_episodes1_episode_id))\n",
    "print(len(final_episodes2_episode_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_pos: 1.0\n",
      "TN:3773 FP:64 FN:626 TP:27 FNR=0.9586523736600306 FPR=0.01667969768047954\n",
      "w_pos: 2.718281828459045\n",
      "TN:3767 FP:70 FN:623 TP:30 FNR=0.9540581929555896 FPR=0.018243419338024498\n",
      "w_pos: 7.38905609893065\n",
      "TN:3744 FP:93 FN:619 TP:34 FNR=0.9479326186830015 FPR=0.024237685691946835\n",
      "w_pos: 20.085536923187668\n",
      "TN:3762 FP:75 FN:621 TP:32 FNR=0.9509954058192955 FPR=0.019546520719311962\n",
      "w_pos: 54.598150033144236\n",
      "TN:3761 FP:76 FN:626 TP:27 FNR=0.9586523736600306 FPR=0.019807140995569454\n",
      "w_pos: 148.4131591025766\n",
      "TN:3715 FP:122 FN:601 TP:52 FNR=0.9203675344563553 FPR=0.03179567370341413\n",
      "w_pos: 403.4287934927351\n",
      "TN:3422 FP:415 FN:515 TP:138 FNR=0.7886676875957122 FPR=0.10815741464685953\n",
      "w_pos: 1096.6331584284585\n",
      "TN:593 FP:3244 FN:66 TP:587 FNR=0.10107197549770292 FPR=0.8454521761793068\n",
      "w_pos: 2980.9579870417283\n",
      "TN:129 FP:3708 FN:6 TP:647 FNR=0.009188361408882083 FPR=0.9663799843627834\n",
      "w_pos: 8103.083927575384\n",
      "TN:96 FP:3741 FN:3 TP:650 FNR=0.004594180704441042 FPR=0.9749804534792806\n",
      "w_pos: 22026.465794806718\n",
      "TN:99 FP:3738 FN:4 TP:649 FNR=0.006125574272588055 FPR=0.9741985926505082\n",
      "w_pos: 59874.14171519782\n",
      "TN:118 FP:3719 FN:5 TP:648 FNR=0.007656967840735069 FPR=0.9692468074016158\n",
      "w_pos: 162754.79141900392\n",
      "TN:101 FP:3736 FN:3 TP:650 FNR=0.004594180704441042 FPR=0.9736773520979932\n",
      "w_pos: 442413.3920089205\n",
      "TN:106 FP:3731 FN:5 TP:648 FNR=0.007656967840735069 FPR=0.9723742507167058\n",
      "w_pos: 1202604.2841647768\n",
      "TN:103 FP:3734 FN:3 TP:650 FNR=0.004594180704441042 FPR=0.9731561115454782\n",
      "w_pos: 3269017.3724721107\n",
      "TN:93 FP:3744 FN:3 TP:650 FNR=0.004594180704441042 FPR=0.9757623143080532\n",
      "w_pos: 8886110.520507872\n",
      "TN:51 FP:3786 FN:2 TP:651 FNR=0.0030627871362940277 FPR=0.9867083659108679\n",
      "w_pos: 24154952.7535753\n",
      "TN:76 FP:3761 FN:3 TP:650 FNR=0.004594180704441042 FPR=0.9801928590044305\n",
      "w_pos: 65659969.13733051\n",
      "TN:18 FP:3819 FN:1 TP:652 FNR=0.0015313935681470138 FPR=0.9953088350273651\n",
      "w_pos: 178482300.96318725\n",
      "TN:0 FP:3837 FN:0 TP:653 FNR=0.0 FPR=1.0\n",
      "w_pos: 485165195.4097903\n",
      "TN:0 FP:3837 FN:0 TP:653 FNR=0.0 FPR=1.0\n"
     ]
    }
   ],
   "source": [
    "# Find the weights for Random Forest\n",
    "# https://www.kaggle.com/code/christiantheilhave/class-imbalance-with-weighted-random-forest/notebook\n",
    "final_episodes1.columns = [str(col) for col in final_episodes1.columns]\n",
    "\n",
    "#features = final_episodes1.iloc[:, 1:-1].columns\n",
    "#X = final_episodes1[features]\n",
    "\n",
    "X = final_episodes1.iloc[:, 1:-1]\n",
    "yb = final_episodes1['binary_label']\n",
    "\n",
    "# Remove rows with NaN in the target variable\n",
    "#X.columns = X.columns.astype(str)\n",
    "X = X.copy()\n",
    "X.fillna(0, inplace=True)\n",
    "X = X[~yb.isna()]\n",
    "yb = yb.dropna()\n",
    "\n",
    "# Splitting the data into training and testing sets *****\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                yb,\n",
    "                                                test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "def confusion(classifier, X_test, y_test):\n",
    "    y_pred  = classifier.predict(X_test)\n",
    "    return confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "def show(tn,fp,fn,tp):\n",
    "    print(\"TN:\" + str(tn) + \" FP:\" + str(fp) + \" FN:\" + str(fn) + \" TP:\" + str(tp) + \n",
    "          \" FNR=\" + str(fn/(fn+tp)) + \" FPR=\" + str(fp/(fp+tn)))\n",
    "    \n",
    "w_neg = 10**-4\n",
    "w_pos_range = np.exp(np.arange(np.log(1), np.log(10**9)))\n",
    "\n",
    "for w_pos in w_pos_range:\n",
    "    print(\"w_pos: \" + str(w_pos))\n",
    "    show(*confusion(RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10, class_weight={0: w_neg, 1: w_pos}).fit(X_train,y_train),X_test,y_test))\n",
    "\n",
    "\n",
    "# w_neg = 0.0001\n",
    "# w_pos = 1096.63    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one: binary label - version(1): without balancing with weights or oversampling\n",
    "class Binary_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_noreadmission_binary_classifier(self):\n",
    "\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       final_episodes1.columns = [str(col) for col in final_episodes1.columns]\n",
    "\n",
    "       #features = final_episodes1.iloc[:, 1:-1].columns\n",
    "       #X = final_episodes1[features]\n",
    "       \n",
    "       X = final_episodes1.iloc[:, 1:-1]\n",
    "       yb = final_episodes1['binary_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       #X.columns = X.columns.astype(str)\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~yb.isna()]\n",
    "       yb = yb.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        yb,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "       # Store the string identifier of the model\n",
    "       model_type = self.model  \n",
    "\n",
    "       #Creating a classifier without making data balanced\n",
    "       print('Model is',self.model)\n",
    "       if model_type == 'DecisionTreeClassifier':\n",
    "             self.model = DecisionTreeClassifier(random_state=42)\n",
    "       elif model_type == 'LogisticRegression':\n",
    "             self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "       elif model_type == 'RandomForestClassifier':\n",
    "             self.model = RandomForestClassifier(random_state=42)\n",
    "       elif model_type == 'GradientBoostingClassifier':\n",
    "             self.model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "       elif model_type == 'XGBClassifier':\n",
    "             self.model = XGBClassifier(random_state=42)\n",
    "       elif model_type == 'MLPClassifier':\n",
    "             self.model = MLPClassifier(random_state=42, max_iter=1000)\n",
    "\n",
    "      # Training\n",
    "       self.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_BL_df = pd.DataFrame()\n",
    "       predicted_BL_df[\"predicted_Binary_label\"] = self.model.predict(X)\n",
    "\n",
    "\n",
    "       predicted_BL_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes1_episode_id, X, predicted_BL_df[\"predicted_Binary_label\"]]], axis=1)\n",
    "       predicted_BL_df = predicted_BL_df[~pd.isna(predicted_BL_df['predicted_Binary_label'])]\n",
    "       predicted_BL_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Initial_predicted_BL_df.csv\") \n",
    "\n",
    "        # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Binary_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "       # Create a StratifiedKFold cross-validation object *****\n",
    "       X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "       X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "       X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "       y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "       cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "       scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "       # Perform cross-validation on the combined training and validation sets\n",
    "       cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "       # Display cross-validation results\n",
    "       print(\"Cross-Validation Accuracy Scores:\")\n",
    "       print(cv_scores)\n",
    "       print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Binary_report = classification_report(y_test, predictions_test)\n",
    "       print(Binary_report)\n",
    "\n",
    "       return Binary_report, Binary_conf_matrix_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one: binary label - version(2): with balancing with balanced class_weight \n",
    "class Binary_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_noreadmission_binary_classifier(self):\n",
    "\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       final_episodes1.columns = [str(col) for col in final_episodes1.columns]\n",
    "\n",
    "       #features = final_episodes1.iloc[:, 1:-1].columns\n",
    "       #X = final_episodes1[features]\n",
    "       \n",
    "       X = final_episodes1.iloc[:, 1:-1]\n",
    "       yb = final_episodes1['binary_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       #X.columns = X.columns.astype(str)\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~yb.isna()]\n",
    "       yb = yb.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        yb,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "       # Store the string identifier of the model\n",
    "       model_type = self.model  \n",
    "\n",
    "      # Creating a classifier with balancing weights and Training the model\n",
    "       print(model_type)\n",
    "       if model_type == 'DecisionTreeClassifier':\n",
    "            self.model = DecisionTreeClassifier(class_weight = 'balanced', random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'LogisticRegression':\n",
    "            self.model = LogisticRegression(random_state=42, class_weight= 'balanced', max_iter=1000)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'RandomForestClassifier':\n",
    "            self.model = RandomForestClassifier(class_weight= 'balanced', random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'GradientBoostingClassifier':\n",
    "            self.model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            self.model.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "       elif model_type == 'XGBClassifier':\n",
    "            self.model = XGBClassifier(random_state=42)\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            self.model.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "       elif model_type == 'MLPClassifier':\n",
    "            self.model = MLPClassifier(random_state=42,learning_rate ='adaptive', max_iter=1000)\n",
    "            self.model.fit(X_train_scaled, y_train) \n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_BL_df = pd.DataFrame()\n",
    "       predicted_BL_df[\"predicted_Binary_label\"] = self.model.predict(X)\n",
    "\n",
    "\n",
    "       predicted_BL_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes1_episode_id, X, predicted_BL_df[\"predicted_Binary_label\"]]], axis=1)\n",
    "       predicted_BL_df = predicted_BL_df[~pd.isna(predicted_BL_df['predicted_Binary_label'])]\n",
    "       predicted_BL_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Balanced_predicted_BL_df.csv\") \n",
    "\n",
    "       # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Binary_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "       # Create a StratifiedKFold cross-validation object *****\n",
    "       X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "       X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "       X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "       y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "       cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "       scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "       # Perform cross-validation on the combined training and validation sets\n",
    "       cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "       # Display cross-validation results\n",
    "       print(\"Cross-Validation Accuracy Scores:\")\n",
    "       print(cv_scores)\n",
    "       print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Binary_report = classification_report(y_test, predictions_test)\n",
    "       print(Binary_report)\n",
    "\n",
    "       return Binary_report, Binary_conf_matrix_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one: binary label - version(3): with balancing with Custom weighting\n",
    "class Binary_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_noreadmission_binary_classifier(self):\n",
    "\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       final_episodes1.columns = [str(col) for col in final_episodes1.columns]\n",
    "\n",
    "       #features = final_episodes1.iloc[:, 1:-1].columns\n",
    "       #X = final_episodes1[features]\n",
    "       \n",
    "       X = final_episodes1.iloc[:, 1:-1]\n",
    "       yb = final_episodes1['binary_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       #X.columns = X.columns.astype(str)\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~yb.isna()]\n",
    "       yb = yb.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        yb,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "       # Calculate the class weight \n",
    "       # for LogisticRegression\n",
    "       wl1 = (0.95)\n",
    "       wl0 = wl1/(counts[0]/counts[1])\n",
    "       # for decision tree\n",
    "       wd1 = (max_val_bin / counts[1]).round(2)\n",
    "       wd0 = (max_val_bin / counts[0]).round(2)\n",
    "\n",
    "       # for Random Forest (we chose these weights from above)\n",
    "       w_neg = 0.0001\n",
    "       w_pos = 1096.63 \n",
    "\n",
    "       # GradientBoosting\n",
    "       GB1 = (counts[0]/counts[1])\n",
    "       GB0 = (counts[0]/counts[0])\n",
    "      # Create a sample_weight array that is all ones\n",
    "       sample_weight = np.ones(y_train.shape)\n",
    "      # Set the weights for class 0\n",
    "       sample_weight[y_train == 0] = GB0\n",
    "      # Set the weights for class 1\n",
    "       sample_weight[y_train == 1] = GB1\n",
    "\n",
    "       # for XGBClassifier\n",
    "       ratio = float(counts[0] / counts[1])\n",
    "\n",
    "      # learning_rate:[0.01,0.05,0.1]\n",
    "      # n_estimators: np.arange(100,500,100)\n",
    "\n",
    "       # Store the string identifier of the model\n",
    "       model_type = self.model  \n",
    "\n",
    "      # Creating a classifier with balancing weights and Training the model\n",
    "       print(model_type)\n",
    "       if model_type == 'DecisionTreeClassifier':\n",
    "            self.model = DecisionTreeClassifier(class_weight = {0:wd0, 1:wd1}, random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'LogisticRegression':\n",
    "            self.model = LogisticRegression(solver='newton-cg', class_weight={0: wl0, 1: wl1}, max_iter=1000,random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'RandomForestClassifier':\n",
    "            self.model = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10, class_weight={0: w_neg, 1: w_pos})\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'GradientBoostingClassifier':\n",
    "            self.model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "       elif model_type == 'XGBClassifier':\n",
    "            self.model = XGBClassifier(scale_pos_weight=ratio,random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train) \n",
    "          #   sample_weight = compute_sample_weight(class_weight='balanced', y=y_train) # not making so much difference\n",
    "          #   self.model.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "       elif model_type == 'MLPClassifier':\n",
    "            self.model = MLPClassifier(learning_rate ='adaptive', max_iter=1000, random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train) \n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_BL_df = pd.DataFrame()\n",
    "       predicted_BL_df[\"predicted_Binary_label\"] = self.model.predict(X)\n",
    "\n",
    "\n",
    "       predicted_BL_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes1_episode_id, X, predicted_BL_df[\"predicted_Binary_label\"]]], axis=1)\n",
    "       predicted_BL_df = predicted_BL_df[~pd.isna(predicted_BL_df['predicted_Binary_label'])]\n",
    "       predicted_BL_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Custom_weight_predicted_BL_df.csv\") \n",
    "\n",
    "       # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Binary_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "       # Create a StratifiedKFold cross-validation object *****\n",
    "       X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "       X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "       X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "       y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "       cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "       scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "       # Perform cross-validation on the combined training and validation sets\n",
    "       cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "       # Display cross-validation results\n",
    "       print(\"Cross-Validation Accuracy Scores:\")\n",
    "       print(cv_scores)\n",
    "       print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Binary_report = classification_report(y_test, predictions_test)\n",
    "       print(Binary_report)\n",
    "\n",
    "       return Binary_report, Binary_conf_matrix_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one: binary label - version(4): balancing with oversampling\n",
    "class Binary_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_noreadmission_binary_classifier(self):\n",
    "\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       final_episodes1.columns = [str(col) for col in final_episodes1.columns]\n",
    "\n",
    "       #features = final_episodes1.iloc[:, 1:-1].columns\n",
    "       #X = final_episodes1[features]\n",
    "       \n",
    "       X = final_episodes1.iloc[:, 1:-1]\n",
    "       yb = final_episodes1['binary_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       #X.columns = X.columns.astype(str)\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~yb.isna()]\n",
    "       yb = yb.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        yb,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "       #Creating a classifier without making data balanced\n",
    "       print(self.model)\n",
    "       if self.model == 'DecisionTreeClassifier':\n",
    "             self.model = DecisionTreeClassifier(random_state=42)\n",
    "       elif self.model == 'LogisticRegression':\n",
    "             self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "       elif self.model == 'RandomForestClassifier':\n",
    "             self.model = RandomForestClassifier(random_state=42)\n",
    "       elif self.model == 'GradientBoostingClassifier':\n",
    "             self.model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "       elif self.model == 'XGBClassifier':\n",
    "             self.model = XGBClassifier(random_state=42)\n",
    "       elif self.model == 'MLPClassifier':\n",
    "             self.model = MLPClassifier(random_state=42, max_iter=1000)\n",
    "\n",
    "       # Oversampling and fitting\n",
    "       ros = RandomOverSampler(random_state=0)\n",
    "       X_resampled, y_resampled = ros.fit_resample(X, yb)   \n",
    "       X_train_scaled = X_resampled\n",
    "       y_train = y_resampled\n",
    "\n",
    "       self.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_BL_df = pd.DataFrame()\n",
    "       predicted_BL_df[\"predicted_Binary_label\"] = self.model.predict(X)\n",
    "\n",
    "\n",
    "       predicted_BL_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes1_episode_id, X, predicted_BL_df[\"predicted_Binary_label\"]]], axis=1)\n",
    "       predicted_BL_df = predicted_BL_df[~pd.isna(predicted_BL_df['predicted_Binary_label'])]\n",
    "       predicted_BL_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Oversampling_predicted_BL_df.csv\") \n",
    "\n",
    "       # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Binary_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "      #  # Create a StratifiedKFold cross-validation object *****\n",
    "      #  X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "      #  X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "      #  X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "      #  y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "      #  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "      #  scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "      #  # Perform cross-validation on the combined training and validation sets\n",
    "      #  cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "      #  # Display cross-validation results\n",
    "      #  print(\"Cross-Validation Accuracy Scores:\")\n",
    "      #  print(cv_scores)\n",
    "      #  print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Binary_report = classification_report(y_test, predictions_test)\n",
    "       print(Binary_report)\n",
    "\n",
    "       return Binary_report, Binary_conf_matrix_test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step two - multi_label version(1): without balancing with weights or oversampling\n",
    "class Multi_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_multi_classifier(self):\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       features = final_episodes2.iloc[:, 1:-2].columns\n",
    "\n",
    "       X = final_episodes2[features]\n",
    "       \n",
    "       ym= final_episodes2['multi_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "       print(X.shape)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~ym.isna()]\n",
    "       ym = ym.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        ym,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "       #Creating a classifier without making data balanced\n",
    "       print(self.model)\n",
    "       if self.model == 'DecisionTreeClassifier':\n",
    "             self.model = DecisionTreeClassifier(random_state=42)\n",
    "       elif self.model == 'LogisticRegression':\n",
    "             self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "       elif self.model == 'RandomForestClassifier':\n",
    "             self.model = RandomForestClassifier(random_state=42)\n",
    "       elif self.model == 'GradientBoostingClassifier':\n",
    "             self.model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "       elif self.model == 'XGBClassifier':\n",
    "             self.model = XGBClassifier(random_state=42)\n",
    "       elif self.model == 'MLPClassifier':\n",
    "             self.model = MLPClassifier(random_state=42, max_iter=1000)\n",
    "\n",
    "      # Training\n",
    "       self.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_ML_df = pd.DataFrame()\n",
    "       predicted_ML_df[\"predicted_Multi_label\"] = self.model.predict(X)\n",
    "\n",
    "       #X2_with_episodeid = pd.concat([final_episodes2_episode_id,X], axis=1)\n",
    "       predicted_ML_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes2_episode_id, X, predicted_ML_df[\"predicted_Multi_label\"]]], axis=1)\n",
    "       # saving the labeled results for later\n",
    "       predicted_ML_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Initial_predicted_ML_df.csv\") \n",
    "\n",
    "       # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Multi_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "       # Create a StratifiedKFold cross-validation object *****\n",
    "       X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "       X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "       X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "       y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "       cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "       scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "       # Perform cross-validation on the combined training and validation sets\n",
    "       cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "       # Display cross-validation results\n",
    "       print(\"Cross-Validation Accuracy Scores:\")\n",
    "       print(cv_scores)\n",
    "       print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Multi_report = classification_report(y_test, predictions_test)\n",
    "       print(Multi_report)\n",
    "\n",
    "       return Multi_conf_matrix_test, Multi_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step two - multi_label version(2): with balancing with balanced class_weightx\n",
    "class Multi_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_multi_classifier(self):\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       features = final_episodes2.iloc[:, 1:-2].columns\n",
    "\n",
    "       X = final_episodes2[features]\n",
    "       \n",
    "       ym= final_episodes2['multi_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "       print(X.shape)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~ym.isna()]\n",
    "       ym = ym.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        ym,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "       \n",
    "       # Calculate the ratio \n",
    "       ratio = [(max_val_multi / counts[0]).round(2), (max_val_multi / counts[1]).round(2), (max_val_multi / counts[2]).round(2)]\n",
    "\n",
    "       # Store the string identifier of the model\n",
    "       model_type = self.model  \n",
    "\n",
    "      # Creating a classifier with balancing weights and Training the model\n",
    "       print(model_type)\n",
    "       if model_type == 'DecisionTreeClassifier':\n",
    "            self.model = DecisionTreeClassifier(class_weight = 'balanced', random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'LogisticRegression':\n",
    "            self.model = LogisticRegression(random_state=42, class_weight= 'balanced', max_iter=1000)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'RandomForestClassifier':\n",
    "            self.model = RandomForestClassifier(class_weight= 'balanced', random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "       elif model_type == 'GradientBoostingClassifier':\n",
    "            self.model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            self.model.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "       elif model_type == 'XGBClassifier':\n",
    "            self.model = XGBClassifier(random_state=42)\n",
    "            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "            self.model.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "       elif model_type == 'MLPClassifier':\n",
    "            self.model = MLPClassifier(random_state=42,learning_rate ='adaptive', max_iter=1000)\n",
    "            self.model.fit(X_train_scaled, y_train) \n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_ML_df = pd.DataFrame()\n",
    "       predicted_ML_df[\"predicted_Multi_label\"] = self.model.predict(X)\n",
    "\n",
    "       #X2_with_episodeid = pd.concat([final_episodes2_episode_id,X], axis=1)\n",
    "       predicted_ML_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes2_episode_id, X, predicted_ML_df[\"predicted_Multi_label\"]]], axis=1)\n",
    "       predicted_ML_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Balanced_predicted_ML_df.csv\") \n",
    "\n",
    "       # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Multi_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "       # Create a StratifiedKFold cross-validation object *****\n",
    "       X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "       X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "       X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "       y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "       cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "       scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "       # Perform cross-validation on the combined training and validation sets\n",
    "       cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "       # Display cross-validation results\n",
    "       print(\"Cross-Validation Accuracy Scores:\")\n",
    "       print(cv_scores)\n",
    "       print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Multi_report = classification_report(y_test, predictions_test)\n",
    "       print(Multi_report)\n",
    "\n",
    "       return Multi_conf_matrix_test, Multi_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step two - multi_label version(3): with balancing with oversampling\n",
    "class Multi_classifier:\n",
    "    def __init__(self, model):\n",
    "       self.model = model\n",
    "       \n",
    "\n",
    "    def readmission_multi_classifier(self):\n",
    "      #  features = ['Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "      #         'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "      #         'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']\n",
    "       features = final_episodes2.iloc[:, 1:-2].columns\n",
    "\n",
    "       X = final_episodes2[features]\n",
    "       \n",
    "       ym= final_episodes2['multi_label']\n",
    "\n",
    "       # Features in use: \n",
    "       print(\"Features:\\n\", X.columns)\n",
    "       print(X.shape)\n",
    "\n",
    "       # Remove rows with NaN in the target variable\n",
    "       X = X.copy()\n",
    "       X.fillna(0, inplace=True)\n",
    "       X = X[~ym.isna()]\n",
    "       ym = ym.dropna()\n",
    "\n",
    "       # Splitting the data into training and testing sets *****\n",
    "       X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                        ym,\n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "       # Split the remaining data into validation and test sets\n",
    "       X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=42)\n",
    "\n",
    "       # Scaling the features\n",
    "       scaler = StandardScaler()\n",
    "       X_train_scaled = scaler.fit_transform(X_train)\n",
    "       X_val_scaled = scaler.transform(X_val)\n",
    "       X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "       #Creating a classifier without making data balanced\n",
    "       print(self.model)\n",
    "       if self.model == 'DecisionTreeClassifier':\n",
    "             self.model = DecisionTreeClassifier(random_state=42)\n",
    "       elif self.model == 'LogisticRegression':\n",
    "             self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "       elif self.model == 'RandomForestClassifier':\n",
    "             self.model = RandomForestClassifier(random_state=42)\n",
    "       elif self.model == 'GradientBoostingClassifier':\n",
    "             self.model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "       elif self.model == 'XGBClassifier':\n",
    "             self.model = XGBClassifier(random_state=42)\n",
    "       elif self.model == 'MLPClassifier':\n",
    "             self.model = MLPClassifier(random_state=42, max_iter=1000)\n",
    "\n",
    "       # Oversampling and fitting\n",
    "       ros = RandomOverSampler(random_state=0)\n",
    "       X_resampled, y_resampled = ros.fit_resample(X, ym)   \n",
    "       X_train_scaled = X_resampled\n",
    "       y_train = y_resampled\n",
    "\n",
    "       self.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "       # Making predictions on the test set\n",
    "       predictions_val = self.model.predict(X_val_scaled)\n",
    "       predictions_test = self.model.predict(X_test_scaled)\n",
    "\n",
    "       # Predict for whole data and save the DataFrame as a .csv file\n",
    "       predicted_ML_df = pd.DataFrame()\n",
    "       predicted_ML_df[\"predicted_Multi_label\"] = self.model.predict(X)\n",
    "\n",
    "       #X2_with_episodeid = pd.concat([final_episodes2_episode_id,X], axis=1)\n",
    "       predicted_ML_df = pd.concat([df.reset_index(drop=True) for df in [final_episodes2_episode_id, X, predicted_ML_df[\"predicted_Multi_label\"]]], axis=1)\n",
    "       predicted_ML_df.to_csv(\"/home/kabank/work/workbench/dipendrp/new-data/Oversampling_predicted_ML_df.csv\") \n",
    "\n",
    "       # Evaluate the model\n",
    "       balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "       f1_val = f1_score(y_val, predictions_val, average='weighted')  \n",
    "       conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "       balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "       f1_test = f1_score(y_test, predictions_test, average='weighted') \n",
    "       conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "       print(\"This is the results:\")\n",
    "       print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "       print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "       print(\"Confusion Matrix_val:\")\n",
    "       print(conf_matrix_val)\n",
    "       print(\"Confusion Matrix_test:\")\n",
    "       Multi_conf_matrix_test = conf_matrix_test\n",
    "       print(conf_matrix_test)\n",
    "\n",
    "      #  # Create a StratifiedKFold cross-validation object *****\n",
    "      #  X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "      #  X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "      #  X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "      #  y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "      #  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "      #  scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "      #  # Perform cross-validation on the combined training and validation sets\n",
    "      #  cv_scores = cross_val_score(self.model, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "      #  # Display cross-validation results\n",
    "      #  print(\"Cross-Validation Accuracy Scores:\")\n",
    "      #  print(cv_scores)\n",
    "      #  print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "       print(f\"f1_val: {f1_val}\")\n",
    "       print(f\"f1_test: {f1_test}\")\n",
    "       # Generate the classification report\n",
    "       Multi_report = classification_report(y_test, predictions_test)\n",
    "       print(Multi_report)\n",
    "\n",
    "       return Multi_conf_matrix_test, Multi_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Features:\n",
      " Index(['Length_of_Episode', 'Count_visit', 'SD_CareEvent_PerMonth',\n",
      "       'num_diagnoses', 'num_medications', 'Inpatient_day_ratio',\n",
      "       'Outpatient_ratio', 'Therapy_ratio', 'TreatmentPlanning_ratio',\n",
      "       'Advisory_ratio',\n",
      "       ...\n",
      "       'G03DA02', 'B03BA03', 'G03FB05', 'S01AA13', 'A11JB', 'M01AE01',\n",
      "       'N05AE04', 'N06AA09', 'R03AC03', 'B03BA01'],\n",
      "      dtype='object', length=216)\n",
      "(3426, 216)\n",
      "LogisticRegression\n",
      "This is the results:\n",
      "Balanced Accuracy (Validation): 0.507104224297291\n",
      "Balanced Accuracy (Test): 0.48369590826985\n",
      "Confusion Matrix_val:\n",
      "[[51 18 29]\n",
      " [46 34 62]\n",
      " [13 13 83]]\n",
      "Confusion Matrix_test:\n",
      "[[106  30  74]\n",
      " [ 79  59 117]\n",
      " [ 34  27 153]]\n",
      "f1_val: 0.454559974391343\n",
      "f1_test: 0.445119563740356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49       210\n",
      "           1       0.51      0.23      0.32       255\n",
      "           2       0.44      0.71      0.55       214\n",
      "\n",
      "    accuracy                           0.47       679\n",
      "   macro avg       0.48      0.48      0.45       679\n",
      "weighted avg       0.48      0.47      0.45       679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kabank/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/kabank/anaconda3/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/kabank/anaconda3/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#options for algorithms: DecisionTreeClassifier, LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, MLPClassifier\n",
    "\n",
    "binary_classifier_ins = Binary_classifier('XGBClassifier').readmission_noreadmission_binary_classifier()\n",
    "print(\"\\n\")\n",
    "multi_classifier_ins = Multi_classifier('LogisticRegression').readmission_multi_classifier()\n",
    "\n",
    "#Final choices:  \n",
    "#Binary_classifier --> XGBClassifier with class weight\n",
    "#Multi_classifier --> Logistic Regression with oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22676\n",
      "predicted_Binary_label\n",
      "0    22450\n",
      "1      226\n",
      "Name: count, dtype: int64\n",
      "3426\n",
      "predicted_Multi_label\n",
      "2    1639\n",
      "0    1076\n",
      "1     711\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the created CSVs\n",
    "# df_I_BL = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Initial_predicted_BL_df.csv')\n",
    "df_B_BL = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Balanced_predicted_BL_df.csv') # the binary model which became balanced by XGBClassifier with class weight\n",
    "# df_CW_BL = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Custom_weight_predicted_BL_df.csv')\n",
    "# df_OS_BL = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Oversampling_predicted_BL_df.csv')\n",
    "print(len(df_B_BL))\n",
    "print(df_B_BL['predicted_Binary_label'].value_counts())\n",
    "# print(df_BL.head())\n",
    "\n",
    "# df_I_ML = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Initial_predicted_ML_df.csv')\n",
    "# df_B_ML = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Balanced_predicted_ML_df.csv')\n",
    "# df_CW_ML = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Custom_weight_predicted_ML_df.csv')\n",
    "df_OS_ML = pd.read_csv('/home/kabank/work/workbench/dipendrp/new-data/Oversampling_predicted_ML_df.csv') # the multi-class model which became oversampled by Logistic Regression with oversampling\n",
    "print(len(df_OS_ML))\n",
    "# print(df_ML.columns)\n",
    "print(df_OS_ML['predicted_Multi_label'].value_counts())\n",
    "# print(df_ML['episode_id'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22676\n",
      "22676\n",
      "3426\n",
      "22676\n",
      "3426\n"
     ]
    }
   ],
   "source": [
    "# check the length of created dataframes\n",
    "print(len(final_episodes))\n",
    "print(len(final_episodes1))\n",
    "print(len(final_episodes2))\n",
    "print(len(final_episodes1_episode_id))\n",
    "print(len(final_episodes2_episode_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with initial solution\n",
    "in the initial solution, we gave these six labels to the data: <br>\n",
    "labels' value count: label <br>\n",
    "5    19250 <br>\n",
    "0      878 <br>\n",
    "1      721 <br>\n",
    "2      718 <br>\n",
    "4      662 <br>\n",
    "3      447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels' value count: label\n",
      "5    19250\n",
      "0      878\n",
      "1      721\n",
      "2      718\n",
      "4      662\n",
      "3      447\n",
      "Name: count, dtype: int64\n",
      "[5 1 4 2 3 0]\n",
      "Features:\n",
      " Index(['Length_of_Episode', 'Count_visit', 'SD_CareEvent_PerMonth',\n",
      "       'num_diagnoses', 'num_medications', 'Inpatient_day_ratio',\n",
      "       'Outpatient_ratio', 'Therapy_ratio', 'TreatmentPlanning_ratio',\n",
      "       'Advisory_ratio',\n",
      "       ...\n",
      "       'G03DA02', 'B03BA03', 'G03FB05', 'S01AA13', 'A11JB', 'M01AE01',\n",
      "       'N05AE04', 'N06AA09', 'R03AC03', 'B03BA01'],\n",
      "      dtype='object', length=216)\n",
      "This is the results for With diagnoses and medications and with DecisionTreeClassifier:\n",
      "Balanced Accuracy (Validation): 0.1777054776183337\n",
      "Balanced Accuracy (Test): 0.18324424962216312\n",
      "Confusion Matrix_val:\n",
      "[[   1    2    1    0    0   88]\n",
      " [   1    1    3    0    2   80]\n",
      " [   1    1    0    1    3   50]\n",
      " [   1    2    1    2    1   43]\n",
      " [   1    0    0    1    3   58]\n",
      " [  25   15   20   11   13 1836]]\n",
      "Confusion Matrix_test:\n",
      "[[   3    1    4    2    0   80]\n",
      " [   1    2    2    1    0   60]\n",
      " [   1    0    1    3    1   65]\n",
      " [   1    0    3    0    0   46]\n",
      " [   0    1    4    1    4   60]\n",
      " [  17   13   17    5   16 1853]]\n",
      "Cross-Validation Accuracy Scores:\n",
      "[0.17631847 0.17146968 0.17220035 0.17493354 0.17028667]\n",
      "Mean Accuracy: 0.17304174152985047\n",
      "f1_val: 0.7675324228975522\n",
      "f1_test: 0.7752548807713595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.03      0.05        90\n",
      "           1       0.12      0.03      0.05        66\n",
      "           2       0.03      0.01      0.02        71\n",
      "           3       0.00      0.00      0.00        50\n",
      "           4       0.19      0.06      0.09        70\n",
      "           5       0.86      0.96      0.91      1921\n",
      "\n",
      "    accuracy                           0.82      2268\n",
      "   macro avg       0.22      0.18      0.19      2268\n",
      "weighted avg       0.74      0.82      0.78      2268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data *****\n",
    "final_episodes = pd.read_csv(\"/home/kabank/workbench/.conda/analysis/kabank-data/final_episodes.csv\")\n",
    "fullHot_episodes = pd.read_csv(\"/home/kabank/workbench/.conda/analysis/kabank-data/final_episodes4.csv\") \n",
    "Diag_med = pd.read_csv('/home/kabank/workbench/.conda/analysis/kabank-data/new-data/Dummies_ICD10_ATC_100.csv') #Using the columns for diagnoses (in ICD-10) and medications (in ATC level 5)\n",
    "\n",
    "# required changes *****\n",
    "final_episodes.rename(columns={'var_no_dates_permonth': 'SD_CareEvent_PerMonth'}, inplace=True)\n",
    "\n",
    "\n",
    "#merge to add some required columns *****\n",
    "final_episodes = pd.merge(final_episodes, fullHot_episodes[['episode_id', 'num_diagnoses', 'num_medications', 'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']], on='episode_id', how='inner')\n",
    "final_episodes = pd.merge(final_episodes[['episode_id','tillnextepisode','Length_of_Episode','Count_visit','SD_CareEvent_PerMonth', 'num_diagnoses', 'num_medications',\n",
    "       'Inpatient_day_ratio', 'Outpatient_ratio', 'Therapy_ratio','TreatmentPlanning_ratio', 'Advisory_ratio',\n",
    "       'gender_0','F','M','MiddleChildhood','Preschooler','Teenager']], Diag_med, on='episode_id', how='left')\n",
    "\n",
    "# Encoding 'tillnextepisode' into labels:  \n",
    "le = LabelEncoder()\n",
    "final_episodes['label'] = le.fit_transform(pd.cut(final_episodes['tillnextepisode'],\n",
    "                                                  bins=[float('-inf'), 0, 180, 365, 730, 1095, float('inf')],\n",
    "                                                  labels=['not-re-admitted',\n",
    "                                                          're-admitted in 0-180 days',\n",
    "                                                          're-admitted in 180-365 days',\n",
    "                                                          're-admitted in 365-730 days',\n",
    "                                                          're-admitted in 730-1095 days',\n",
    "                                                          're-admitted in more than 1095 days']))\n",
    "# labels with value counts *****\n",
    "print(\"labels' value count:\", final_episodes['label'].value_counts())\n",
    "print(final_episodes['label'].unique())\n",
    "\n",
    "# Selecting independent features\n",
    "features = final_episodes.iloc[:, 2:-1].columns\n",
    "\n",
    "X = final_episodes[features]\n",
    "y= final_episodes['label']\n",
    "\n",
    "# Features in use: \n",
    "print(\"Features:\\n\", X.columns)\n",
    "\n",
    "# Remove rows with NaN in the target variable\n",
    "X = X.copy()\n",
    "X.fillna(0, inplace=True)\n",
    "X = X[~y.isna()]\n",
    "y = y.dropna()\n",
    "\n",
    "# Splitting the data into training and testing sets *****\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y),y=y)\n",
    "class_weight = dict(zip(np.unique(final_episodes['label']), class_weights))\n",
    "\n",
    "# Creating a decision tree classifier with class weights\n",
    "NN = MLPClassifier(random_state=42,learning_rate ='adaptive', max_iter=1000)\n",
    "\n",
    "# Training the model\n",
    "NN.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions_val = NN.predict(X_val_scaled)\n",
    "predictions_test = NN.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "balanced_accuracy_val = balanced_accuracy_score(y_val, predictions_val)\n",
    "f1_val = f1_score(y_val, predictions_val, average='weighted')  # Using 'weighted' for multiclass problems\n",
    "conf_matrix_val = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test, predictions_test)\n",
    "f1_test = f1_score(y_test, predictions_test, average='weighted')  # Using 'weighted' for multiclass problems\n",
    "conf_matrix_test = confusion_matrix(y_test, predictions_test)\n",
    "\n",
    "\n",
    "print(\"This is the results for With diagnoses and medications and with DecisionTreeClassifier:\")\n",
    "print(f\"Balanced Accuracy (Validation): {balanced_accuracy_val}\")\n",
    "print(f\"Balanced Accuracy (Test): {balanced_accuracy_test}\")\n",
    "\n",
    "print(\"Confusion Matrix_val:\")\n",
    "print(conf_matrix_val)\n",
    "print(\"Confusion Matrix_test:\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "# Create a StratifiedKFold cross-validation object *****\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled)\n",
    "\n",
    "X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Perform cross-validation on the combined training and validation sets\n",
    "cv_scores = cross_val_score(NN, X_train_val_scaled, y_train_val, cv=cv, scoring=scorer)\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"Cross-Validation Accuracy Scores:\")\n",
    "print(cv_scores)\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(f\"f1_val: {f1_val}\")\n",
    "print(f\"f1_test: {f1_test}\")\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, predictions_test)\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Kernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
